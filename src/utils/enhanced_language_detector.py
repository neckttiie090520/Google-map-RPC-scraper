#!/usr/bin/env python3
"""
Enhanced Language Detector for Chinese Variants
==============================================

This module provides enhanced language detection that can distinguish between
different Chinese language variants and other Asian languages more accurately.

Supported Chinese Variants:
- zh-cn: Simplified Chinese (ç®€ä½“ä¸­æ–‡)
- zh-tw: Traditional Chinese (ç¹é«”ä¸­æ–‡)
- zh-hk: Hong Kong Chinese (é¦™æ¸¯ä¸­æ–‡)
- zh-sg: Singapore Chinese
- zh-my: Malaysian Chinese

Other Enhanced Detection:
- ja: Japanese (æ—¥æœ¬èª)
- ko: Korean (í•œêµ­ì–´)
- th: Thai (à¹„à¸—à¸¢)
- en: English
- id: Indonesian
- vi: Vietnamese
- ms: Malay
"""

import re
from typing import Dict, List, Optional, Tuple
from langdetect import detect, DetectorFactory
import logging

# Set seed for consistent detection
DetectorFactory.seed = 0

class EnhancedLanguageDetector:
    """Enhanced language detector with Chinese variant support"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # Chinese character patterns for different variants
        self.chinese_patterns = {
            'zh-cn': {
                # Simplified Chinese characters (common examples)
                'characters': ['ä¸­', 'å›½', 'æ–‡', 'æ¥', 'ä¸ª', 'å­¦', 'å¼€', 'å…³', 'é•¿', 'ä¸œ', 'è´', 'è½¦', 'è§', 'ä¹°', 'å–'],
                'words': ['ä¸­å›½', 'ä¸­æ–‡', 'å­¦ä¹ ', 'å·¥ä½œ', 'å…¬å¸', 'å‘å±•', 'ç»æµ', 'ç½‘ç»œ', 'æ‰‹æœº', 'ç”µè„‘'],
                'description': 'Simplified Chinese (ç®€ä½“ä¸­æ–‡)'
            },
            'zh-tw': {
                # Traditional Chinese characters (common examples)
                'characters': ['ä¸­', 'åœ‹', 'æ–‡', 'ä¾†', 'å€‹', 'å­¸', 'é–‹', 'é—œ', 'é•·', 'æ±', 'è²', 'è»Š', 'è¦‹', 'è²·', 'è³£'],
                'words': ['ä¸­åœ‹', 'ä¸­æ–‡', 'å­¸ç¿’', 'å·¥ä½œ', 'å…¬å¸', 'ç™¼å±•', 'ç¶“æ¿Ÿ', 'ç¶²çµ¡', 'æ‰‹æ©Ÿ', 'é›»è…¦'],
                'description': 'Traditional Chinese (ç¹é«”ä¸­æ–‡)'
            },
            'zh-hk': {
                # Hong Kong specific patterns
                'characters': ['ä¸­', 'åœ‹', 'æ–‡', 'ä¾†', 'å€‹', 'å­¸', 'é–‹', 'é—œ', 'é•·', 'æ±'],
                'words': ['é¦™æ¸¯', 'æ¾³é–€', 'å»£æ±', 'è©±', 'é£²èŒ¶', 'é»å¿ƒ', 'å·´å£«', 'çš„å£«'],
                'description': 'Hong Kong Chinese (é¦™æ¸¯ä¸­æ–‡)'
            }
        }

        # Other language patterns for better accuracy
        self.language_patterns = {
            'ja': {
                'characters': ['ã‚', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ã', 'ã', 'ã‘', 'ã“', 'æ¼¢å­—', 'ã²ã‚‰ãŒãª', 'ã‚«ã‚¿ã‚«ãƒŠ'],
                'patterns': [r'[ã²ã‚‰ãŒãª]', r'[ã‚«ã‚¿ã‚«ãƒŠ]', r'ã§ã™', r'ã¾ã™'],
                'description': 'Japanese (æ—¥æœ¬èª)'
            },
            'ko': {
                'characters': ['ê°€', 'ë‚˜', 'ë‹¤', 'ë¼', 'ë§ˆ', 'ë°”', 'ì‚¬', 'ì•„', 'ì', 'ì°¨'],
                'patterns': [r'[ê°€-í£]', r'í•©ë‹ˆë‹¤', r'ì…ë‹ˆë‹¤', r'ã…‚ë‹ˆë‹¤', r'ã…‚ë‹ˆë‹¤'],
                'description': 'Korean (í•œêµ­ì–´)'
            },
            'th': {
                'characters': ['à¸', 'à¸‚', 'à¸ƒ', 'à¸„', 'à¸…', 'à¸†', 'à¸‡', 'à¸ˆ', 'à¸‰', 'à¸Š'],
                'patterns': [r'[à¸-à¹›]', r'à¸„à¸£à¸±à¸š', r'à¸„à¹ˆà¸°', r'à¸™à¸°à¸„à¸°', r'à¹€à¸£à¸µà¸¢à¸šà¸£à¹‰à¸­à¸¢'],
                'description': 'Thai (à¹„à¸—à¸¢)'
            }
        }

    def detect_chinese_variant(self, text: str) -> Optional[str]:
        """
        Detect Chinese variant with enhanced accuracy

        Args:
            text: Text to analyze

        Returns:
            Chinese language code (zh-cn, zh-tw, zh-hk) or None if not Chinese
        """
        if not text or len(text.strip()) < 2:
            return None

        text_clean = text.strip()

        # Check for Hong Kong specific patterns first
        hk_words = ['é¦™æ¸¯', 'æ¾³é–€', 'å»£æ±è©±', 'é£²èŒ¶', 'é»å¿ƒ', 'å·´å£«', 'çš„å£«', 'èŒ¶é¤å»³']
        if any(word in text_clean for word in hk_words):
            return 'zh-hk'

        # Count simplified vs traditional characters
        simplified_count = 0
        traditional_count = 0

        # Common simplified/traditional pairs
        char_pairs = [
            ('å›½', 'åœ‹'), ('å­¦', 'å­¸'), ('å¼€', 'é–‹'), ('å…³', 'é—œ'), ('é•¿', 'é•·'),
            ('ä¸œ', 'æ±'), ('è´', 'è²'), ('è½¦', 'è»Š'), ('è§', 'è¦‹'), ('ä¹°', 'è²·'),
            ('å–', 'è³£'), ('ä¸ª', 'å€‹'), ('æ¥', 'ä¾†'), ('å‘', 'ç™¼'), ('ä¼š', 'æœƒ'),
            ('æœº', 'æ©Ÿ'), ('ç”µ', 'é›»'), ('è„‘', 'è…¦'), ('é•¿', 'é•·'), ('é—¨', 'é–€')
        ]

        for simple, traditional in char_pairs:
            simplified_count += text_clean.count(simple)
            traditional_count += text_clean.count(traditional)

        # Additional variant-specific characters
        simplified_chars = ['ä¸ª', 'é•¿', 'å‘', 'æœº', 'ç”µ', 'è¿™', 'é‚£', 'å¥¹', 'å®ƒ', 'ä»¬']
        traditional_chars = ['å€‹', 'é•·', 'ç™¼', 'æ©Ÿ', 'é›»', 'é€™', 'é‚£', 'å¥¹', 'å®ƒ', 'å€‘']

        for char in simplified_chars:
            simplified_count += text_clean.count(char)

        for char in traditional_chars:
            traditional_count += text_clean.count(char)

        total_chinese = simplified_count + traditional_count

        if total_chinese == 0:
            return None

        # Determine variant based on character ratio
        if total_chinese > 0:
            simplified_ratio = simplified_count / total_chinese

            if simplified_ratio > 0.7:
                return 'zh-cn'  # Predominantly simplified
            elif simplified_ratio < 0.3:
                return 'zh-tw'  # Predominantly traditional
            else:
                # Mixed case - use additional heuristics
                # Check for common words in each variant
                simplified_words = ['æˆ‘ä»¬', 'å­¦ä¹ ', 'å·¥ä½œ', 'å…¬å¸', 'å‘å±•', 'ç½‘ç»œ']
                traditional_words = ['æˆ‘å€‘', 'å­¸ç¿’', 'å·¥ä½œ', 'å…¬å¸', 'ç™¼å±•', 'ç¶²çµ¡']

                simplified_word_count = sum(1 for word in simplified_words if word in text_clean)
                traditional_word_count = sum(1 for word in traditional_words if word in text_clean)

                if simplified_word_count > traditional_word_count:
                    return 'zh-cn'
                elif traditional_word_count > simplified_word_count:
                    return 'zh-tw'
                else:
                    # Default to simplified for mixed cases
                    return 'zh-cn'

        return None

    def detect_language_enhanced(self, text: str) -> str:
        """
        Detect language with enhanced Chinese variant support

        Args:
            text: Text to detect language for

        Returns:
            Language code with variant support (e.g., 'zh-cn', 'zh-tw', 'ja', 'ko', 'th', 'en')
        """
        if not text or not text.strip():
            return 'unknown'

        text_clean = text.strip()

        # First check for Chinese variants
        chinese_variant = self.detect_chinese_variant(text_clean)
        if chinese_variant:
            return chinese_variant

        # Check other languages with pattern matching
        for lang_code, patterns in self.language_patterns.items():
            if 'patterns' in patterns:
                for pattern in patterns['patterns']:
                    if re.search(pattern, text_clean):
                        return lang_code
            elif 'characters' in patterns:
                # Check for presence of language-specific characters
                char_count = sum(1 for char in text_clean if char in patterns['characters'])
                if char_count > len(text_clean) * 0.1:  # 10% threshold
                    return lang_code

        # Fallback to standard langdetect
        try:
            detected = detect(text_clean[:500])  # Use first 500 chars

            # Map generic Chinese to specific variant
            if detected == 'zh':
                # Use Chinese variant detection as fallback
                variant = self.detect_chinese_variant(text_clean)
                return variant if variant else 'zh-cn'  # Default to simplified

            return detected

        except Exception as e:
            self.logger.debug(f"Language detection failed: {e}")
            return 'unknown'

    def get_language_name(self, lang_code: str) -> str:
        """
        Get human-readable language name with variant support

        Args:
            lang_code: Language code (e.g., 'zh-cn', 'zh-tw', 'ja', 'th')

        Returns:
            Human-readable language name in Thai
        """
        language_names = {
            # Chinese variants
            'zh-cn': 'à¸ˆà¸µà¸™à¸•à¸±à¸§à¸¢à¹ˆà¸­ (Simplified)',
            'zh-tw': 'à¸ˆà¸µà¸™à¸•à¸±à¸§à¹€à¸•à¹‡à¸¡ (Traditional)',
            'zh-hk': 'à¸ˆà¸µà¸™à¸®à¹ˆà¸­à¸‡à¸à¸‡ (Hong Kong)',
            'zh-sg': 'à¸ˆà¸µà¸™à¸ªà¸´à¸‡à¸„à¹‚à¸›à¸£à¹Œ',
            'zh-my': 'à¸ˆà¸µà¸™à¸¡à¸²à¹€à¸¥à¹€à¸‹à¸µà¸¢',
            'zh': 'à¸ˆà¸µà¸™',

            # Other languages
            'en': 'à¸­à¸±à¸‡à¸à¸¤à¸©',
            'ja': 'à¸à¸µà¹ˆà¸›à¸¸à¹ˆà¸™',
            'ko': 'à¹€à¸à¸²à¸«à¸¥à¸µ',
            'th': 'à¹„à¸—à¸¢',
            'id': 'à¸­à¸´à¸™à¹‚à¸”à¸™à¸µà¹€à¸‹à¸µà¸¢',
            'vi': 'à¹€à¸§à¸µà¸¢à¸”à¸™à¸²à¸¡',
            'ms': 'à¸¡à¸²à¹€à¸¥à¸¢à¹Œ',
            'es': 'à¸ªà¹€à¸›à¸™',
            'fr': 'à¸à¸£à¸±à¹ˆà¸‡à¹€à¸¨à¸ª',
            'de': 'à¹€à¸¢à¸­à¸£à¸¡à¸±à¸™',
            'ru': 'à¸£à¸±à¸ªà¹€à¸‹à¸µà¸¢',
            'ar': 'à¸­à¸²à¸«à¸£à¸±à¸š',
            'hi': 'à¸®à¸´à¸™à¸”à¸µ',
            'pt': 'à¹‚à¸›à¸£à¸•à¸¸à¹€à¸à¸ª',
            'it': 'à¸­à¸´à¸•à¸²à¸¥à¸µ',
            'nl': 'à¸”à¸±à¸•à¸Šà¹Œ',
            'unknown': 'à¹„à¸¡à¹ˆà¸—à¸£à¸²à¸š'
        }

        return language_names.get(lang_code, lang_code.upper())

    def batch_detect_languages(self, texts: List[str]) -> Dict[str, int]:
        """
        Detect languages in batch and return statistics

        Args:
            texts: List of texts to analyze

        Returns:
            Dictionary with language codes as keys and counts as values
        """
        language_stats = {}

        for text in texts:
            if text and text.strip():
                lang = self.detect_language_enhanced(text)
                language_stats[lang] = language_stats.get(lang, 0) + 1

        return language_stats

def create_enhanced_detector() -> EnhancedLanguageDetector:
    """Factory function to create enhanced language detector"""
    return EnhancedLanguageDetector()

# Test function
if __name__ == "__main__":
    detector = create_enhanced_detector()

    # Test texts
    test_texts = [
        ("è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åœ°æ–¹ï¼Œå¼ºçƒˆæ¨èï¼", "zh-cn"),
        ("é€™æ˜¯ä¸€å€‹å¾ˆå¥½çš„åœ°æ–¹ï¼Œå¼·çƒˆæ¨è–¦ï¼", "zh-tw"),
        ("é¦™æ¸¯é€™å€‹åœ°æ–¹ä¸éŒ¯ï¼Œé£²èŒ¶å¾ˆå¥½åƒ", "zh-hk"),
        ("ã“ã®å ´æ‰€ã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ï¼", "ja"),
        ("ì´ ì¥ì†ŒëŠ” í›Œë¥­í•©ë‹ˆë‹¤!", "ko"),
        ("à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆà¸™à¸µà¹‰à¸”à¸µà¸¡à¸²à¸à¸„à¸£à¸±à¸š", "th"),
        ("This is a great place!", "en"),
        ("Ini adalah tempat yang bagus!", "id"),
        ("ÄÃ¢y lÃ  má»™t nÆ¡i tuyá»‡t vá»i!", "vi")
    ]

    print("ğŸ§ª Testing Enhanced Language Detection")
    print("=" * 50)

    for text, expected in test_texts:
        detected = detector.detect_language_enhanced(text)
        name = detector.get_language_name(detected)
        expected_name = detector.get_language_name(expected)

        status = "âœ…" if detected == expected else "âŒ"
        print(f"{status} {name} | Expected: {expected_name}")
        print(f"   Text: {text[:50]}...")
        print()

    # Test batch detection
    print("ğŸ“Š Batch Detection Test")
    print("=" * 50)

    batch_texts = [text for text, _ in test_texts] * 3  # Repeat for better stats
    stats = detector.batch_detect_languages(batch_texts)

    print("Language Detection Results:")
    for lang, count in sorted(stats.items(), key=lambda x: x[1], reverse=True):
        name = detector.get_language_name(lang)
        print(f"  {name}: {count}")